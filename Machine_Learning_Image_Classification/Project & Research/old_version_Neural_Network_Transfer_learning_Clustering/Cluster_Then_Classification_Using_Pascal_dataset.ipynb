{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cluster-Then-Classification-Using Pascal dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqvDWiuIX2R5"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "import os\n",
        "import numpy\n",
        "import glob\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8UPdBzJncym",
        "outputId": "2cbaf8ef-e3bc-42d4-e4f1-e0d8cb288529"
      },
      "source": [
        "#Gets Dataset Image data and classes\n",
        "def loadDataSubSet(imageDirectory, imageSetPath):\n",
        "    NUM_PIXELS = 49152 #128x128 pixels * 3 values for RGB\n",
        "    CLASSES = ['aeroplane', 'bicycle', 'bird', 'boat', \n",
        "                  'bottle', 'bus', 'car', 'cat', 'chair',\n",
        "                  'cow', 'diningtable', 'dog', 'horse',\n",
        "                  'motorbike', 'person', 'plottedplant',\n",
        "                  'sheep', 'sofa', 'train', 'tvmonitor'] \n",
        "\n",
        "    #Open ImageSet File\n",
        "    imageSetFile = open(imageSetPath, 'r')\n",
        "    imgsInSet = [] \n",
        "    \n",
        "    print(\"Starting to load subset of images in: \",imageSetPath)\n",
        "\n",
        "    #Read in all Images in the ImageSet\n",
        "    while (True):\n",
        "        line = imageSetFile.readline().splitlines()\n",
        "        #If end line - exit loop \n",
        "        if not line: \n",
        "            break\n",
        "\n",
        "        #Convert the file name to a clean path to the associated file\n",
        "        cleanLine = str(line)[1:-1].replace('\\'', '')\n",
        "        cleanPath = os.path.join(imageDirectory,'*/{}.png'.format(cleanLine))\n",
        "        fullPath = glob.glob(cleanPath)\n",
        "        cleanFullPath = str(fullPath)[1:-1].replace('\\'', '')\n",
        "        imgsInSet.append(cleanFullPath)\n",
        "        #print(cleanFullPath)\n",
        "\n",
        "    print(\"Starting to process images in: \",imageSetPath)\n",
        "\n",
        "    #Load the images\n",
        "    #Initialize the containers\n",
        "    numImages = len(imgsInSet)\n",
        "    x_data = numpy.empty([numImages, NUM_PIXELS])\n",
        "    y_data = numpy.empty([numImages,])\n",
        "\n",
        "    #For each image, get image in appropriate format for x_data \n",
        "        #and class for y_data\n",
        "    i = 0\n",
        "    for filename in imgsInSet:\n",
        "        #Reshape data from image file\n",
        "        x_data[i] = asarray(Image.open(filename)).flatten().reshape(1, -1)\n",
        "        \n",
        "        #Identify the class that the image is part of. Convert to int\n",
        "        filePath = os.path.dirname(filename)\n",
        "        className = os.path.basename(filePath)\n",
        "        j = 0\n",
        "        for c in CLASSES:\n",
        "            if (c == className):\n",
        "                y_data[i] = j\n",
        "                break\n",
        "            j = j + 1\n",
        "\n",
        "        i = i + 1\n",
        "    print(\"Done: \",imageSetPath)\n",
        "    #print(x_data)\n",
        "    y_data = y_data.astype(int)\n",
        "    #print(y_data)\n",
        "    return x_data, y_data\n",
        "\n",
        "#Test loading digits dataset for comparison\n",
        "def loadImagesTest():\n",
        "    digits = load_digits()\n",
        "    X_digits, y_digits = load_digits(return_X_y=True)\n",
        "    print(\"[loadImagesTest]: X_digits Type: \",format(type(X_digits)))\n",
        "    print(\"[loadImagesTest]: X_digits Shape: \",format(X_digits.shape))\n",
        "    print(\"[loadImagesTest]: X_digit: \",format(X_digits))\n",
        "\n",
        "    print(\"[loadImagesTest]: y_digits Type: \",format(type(y_digits)))\n",
        "    print(\"[loadImagesTest]: y_digits Shape: \",format(y_digits.shape))\n",
        "    print(\"[loadImagesTest]: y_digit: \",format(y_digits))\n",
        "    #print(\"feature_names: \", format(digits.feature_names))\n",
        "    #print(\"target_names: \", format(digits.target_names))\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)\n",
        "    print(X_train)\n",
        "    print(X_test)\n",
        "    print(y_train)\n",
        "    print(y_test)\n",
        "\n",
        "#loadImagesTest()\n",
        "\n",
        "#Save Resized images and Imagesets to google drive\n",
        "# drive.mount('https://drive.google.com/drive/shared-with-me')\n",
        "# imageDirectory = r'drive/u/0/shared-with-me/ResizedPNGImagesSmall'\n",
        "# imageSetsDirectory = r'drive/u/0/shared-with-me/ImageSets/Main'\n",
        "#Save Resized images and Imagesets to google drive\n",
        "drive.mount('/content/drive')\n",
        "imageDirectory = r'drive/My Drive/ResizedPNGImagesSmall/ResizedPNGImagesSmall'\n",
        "imageSetsDirectory = r'drive/My Drive/ImageSets/ImageSets/Main'\n",
        "\n",
        "\n",
        "imageSetTrainPath = os.path.join(imageSetsDirectory,'train.txt')\n",
        "imageSetValPath = os.path.join(imageSetsDirectory,'val.txt')\n",
        "\n",
        "x_train, y_train = loadDataSubSet(imageDirectory, imageSetTrainPath)\n",
        "x_test, y_test = loadDataSubSet(imageDirectory, imageSetValPath)\n",
        "\n",
        "log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\n",
        "print(\"Training\")\n",
        "print(log_reg.fit(x_train, y_train))\n",
        "print(\"End Training.... Starting Test\")\n",
        "print(log_reg.score(x_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Starting to load subset of images in:  drive/My Drive/ImageSets/ImageSets/Main/train.txt\n",
            "Starting to process images in:  drive/My Drive/ImageSets/ImageSets/Main/train.txt\n",
            "Done:  drive/My Drive/ImageSets/ImageSets/Main/train.txt\n",
            "Starting to load subset of images in:  drive/My Drive/ImageSets/ImageSets/Main/val.txt\n",
            "Starting to process images in:  drive/My Drive/ImageSets/ImageSets/Main/val.txt\n",
            "Done:  drive/My Drive/ImageSets/ImageSets/Main/val.txt\n",
            "Training\n",
            "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
            "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
            "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "End Training.... Starting Test\n",
            "0.13446676970633695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNHCmDbSXfKp"
      },
      "source": [
        "Okay, that's our baseline: 13% accuracy. Let's see if we can do better by using K-Means as a preprocessing step. We will create a pipeline that will first cluster the training set into 70 clusters and replace the images with their distances to the 70 clusters, then apply a logistic regression model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk3TeM1xXUgJ"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0trwIDK6XqbB",
        "outputId": "85b83d2a-1d4a-438b-c405-9358f3857f0f"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    (\"kmeans\", KMeans(n_clusters=70, random_state=42)),\n",
        "    (\"log_reg\", LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)),\n",
        "])\n",
        "pipeline.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('kmeans',\n",
              "                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\n",
              "                        max_iter=300, n_clusters=70, n_init=10, n_jobs=None,\n",
              "                        precompute_distances='auto', random_state=42,\n",
              "                        tol=0.0001, verbose=0)),\n",
              "                ('log_reg',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=5000,\n",
              "                                    multi_class='ovr', n_jobs=None,\n",
              "                                    penalty='l2', random_state=42,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6daQqhEQfam",
        "outputId": "8f7b4631-dfc6-4632-c306-163773f24c3f"
      },
      "source": [
        "pipeline.score(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22084835995191482"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "QFjM3ztCQLOv",
        "outputId": "d2cd730f-2aa4-40a5-879a-28d7e13e11a0"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = dict(kmeans__n_clusters=range(50, 60))\n",
        "grid_clf = GridSearchCV(pipeline, param_grid, verbose=2)\n",
        "grid_clf.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fd713766ee91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans__n_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgrid_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgrid_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "zoDrHsbnQWL-",
        "outputId": "cd8b979c-e3a0-416c-c466-128cc8ad8709"
      },
      "source": [
        "grid_clf.score(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4e4bb00f3130>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'grid_clf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isSww0OCYNto",
        "outputId": "7a23334f-2a1c-4008-a237-edefe0c23fea"
      },
      "source": [
        "pipeline.score(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22084835995191482"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "QqI16twAkr_o",
        "outputId": "2329fa15-c097-4276-e31b-1121d62d2fb9"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    (\"kmeans\", KMeans(n_clusters=100, random_state=42)),\n",
        "    (\"log_reg\", LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)),\n",
        "])\n",
        "pipeline.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-92391dab28dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline = Pipeline([\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"kmeans\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"log_reg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ovr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m ])\n\u001b[1;32m      5\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOEWNomEbc00"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_f4Vcyibc3g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vFgtazfbc5v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3fqdbwwbc8F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jxvYhdvbc-k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd3EGf9hbdBA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReJj6Y6WbdD1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2DN43CnbdGO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQeT-DAdbdIm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9quHnUXbdLC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlz4-HcdbdNR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgGeFR47bdP7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6K-VVETbdR6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV9ICyiKbdU_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odSbvcv9bdXa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcokyoZqbdbs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xheNK-iRbdfD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6aaNBbtbdhy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpevU7jpbdoU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--pHLOy1bdrD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7iwczSnbdtk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6-Rz4nAbdwZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbrqGoU5bdzd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M9rsos1WLrg"
      },
      "source": [
        "# **Building an Image Classifier with Tf and keras using fashion MNIST dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvjgUjtDWEry"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "OFtHlWnkWV0h",
        "outputId": "a8fc3bf6-1326-4cbf-c295-13b551bb5583"
      },
      "source": [
        "#Gets Dataset Image data and classes\n",
        "def loadDataSubSet(imageDirectory, imageSetPath):\n",
        "    NUM_PIXELS = 49152 #128x128 pixels * 3 values for RGB\n",
        "    CLASSES = ['aeroplane', 'bicycle', 'bird', 'boat', \n",
        "                  'bottle', 'bus', 'car', 'cat', 'chair',\n",
        "                  'cow', 'diningtable', 'dog', 'horse',\n",
        "                  'motorbike', 'person', 'plottedplant',\n",
        "                  'sheep', 'sofa', 'train', 'tvmonitor'] \n",
        "\n",
        "    #Open ImageSet File\n",
        "    imageSetFile = open(imageSetPath, 'r')\n",
        "    imgsInSet = [] \n",
        "    \n",
        "    print(\"Starting to load subset of images in: \",imageSetPath)\n",
        "\n",
        "    #Read in all Images in the ImageSet\n",
        "    while (True):\n",
        "        line = imageSetFile.readline().splitlines()\n",
        "        #If end line - exit loop \n",
        "        if not line: \n",
        "            break\n",
        "\n",
        "        #Convert the file name to a clean path to the associated file\n",
        "        cleanLine = str(line)[1:-1].replace('\\'', '')\n",
        "        cleanPath = os.path.join(imageDirectory,'*/{}.png'.format(cleanLine))\n",
        "        fullPath = glob.glob(cleanPath)\n",
        "        cleanFullPath = str(fullPath)[1:-1].replace('\\'', '')\n",
        "        imgsInSet.append(cleanFullPath)\n",
        "        #print(cleanFullPath)\n",
        "\n",
        "    print(\"Starting to process images in: \",imageSetPath)\n",
        "\n",
        "    #Load the images\n",
        "    #Initialize the containers\n",
        "    numImages = len(imgsInSet)\n",
        "    x_data = numpy.empty([numImages, NUM_PIXELS])\n",
        "    y_data = numpy.empty([numImages,])\n",
        "\n",
        "    #For each image, get image in appropriate format for x_data \n",
        "        #and class for y_data\n",
        "    i = 0\n",
        "    for filename in imgsInSet:\n",
        "        #Reshape data from image file\n",
        "        x_data[i] = asarray(Image.open(filename)).flatten().reshape(1, -1)\n",
        "        \n",
        "        #Identify the class that the image is part of. Convert to int\n",
        "        filePath = os.path.dirname(filename)\n",
        "        className = os.path.basename(filePath)\n",
        "        j = 0\n",
        "        for c in CLASSES:\n",
        "            if (c == className):\n",
        "                y_data[i] = j\n",
        "                break\n",
        "            j = j + 1\n",
        "\n",
        "        i = i + 1\n",
        "    print(\"Done: \",imageSetPath)\n",
        "    #print(x_data)\n",
        "    y_data = y_data.astype(int)\n",
        "    #print(y_data)\n",
        "    return x_data, y_data\n",
        "\n",
        "#Test loading digits dataset for comparison\n",
        "def loadImagesTest():\n",
        "    digits = load_digits()\n",
        "    X_digits, y_digits = load_digits(return_X_y=True)\n",
        "    print(\"[loadImagesTest]: X_digits Type: \",format(type(X_digits)))\n",
        "    print(\"[loadImagesTest]: X_digits Shape: \",format(X_digits.shape))\n",
        "    print(\"[loadImagesTest]: X_digit: \",format(X_digits))\n",
        "\n",
        "    print(\"[loadImagesTest]: y_digits Type: \",format(type(y_digits)))\n",
        "    print(\"[loadImagesTest]: y_digits Shape: \",format(y_digits.shape))\n",
        "    print(\"[loadImagesTest]: y_digit: \",format(y_digits))\n",
        "    #print(\"feature_names: \", format(digits.feature_names))\n",
        "    #print(\"target_names: \", format(digits.target_names))\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)\n",
        "    print(X_train)\n",
        "    print(X_test)\n",
        "    print(y_train)\n",
        "    print(y_test)\n",
        "\n",
        "#loadImagesTest()\n",
        "\n",
        "#Save Resized images and Imagesets to google drive\n",
        "# drive.mount('https://drive.google.com/drive/shared-with-me')\n",
        "# imageDirectory = r'drive/u/0/shared-with-me/ResizedPNGImagesSmall'\n",
        "# imageSetsDirectory = r'drive/u/0/shared-with-me/ImageSets/Main'\n",
        "#Save Resized images and Imagesets to google drive\n",
        "drive.mount('/content/drive')\n",
        "imageDirectory = r'drive/My Drive/ResizedPNGImagesSmall/ResizedPNGImagesSmall'\n",
        "imageSetsDirectory = r'drive/My Drive/ImageSets/ImageSets/Main'\n",
        "\n",
        "\n",
        "imageSetTrainPath = os.path.join(imageSetsDirectory,'train.txt')\n",
        "imageSetValPath = os.path.join(imageSetsDirectory,'val.txt')\n",
        "\n",
        "x_train, y_train = loadDataSubSet(imageDirectory, imageSetTrainPath)\n",
        "x_test, y_test = loadDataSubSet(imageDirectory, imageSetValPath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a4404a225086>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# imageSetsDirectory = r'drive/u/0/shared-with-me/ImageSets/Main'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m#Save Resized images and Imagesets to google drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0mimageDirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'drive/My Drive/ResizedPNGImagesSmall/ResizedPNGImagesSmall'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mimageSetsDirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'drive/My Drive/ImageSets/ImageSets/Main'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1DsHcetWxqD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}