{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cluster-Then-Classification using smaller dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "29v1GFRZbL-X"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "import os\n",
        "import numpy\n",
        "import glob\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytiZmvj0bUAV",
        "outputId": "f7fbf6da-ae19-43d3-f64a-b85c961e8cbe"
      },
      "source": [
        "#Gets Dataset Image data and classes\n",
        "def loadDataSubSet(imageDirectory, imageSetPath):\n",
        "    NUM_PIXELS = 49152 #128x128 pixels * 3 values for RGB\n",
        "    CLASSES = ['aeroplane'] \n",
        "\n",
        "    #Open ImageSet File\n",
        "    imageSetFile = open(imageSetPath, 'r')\n",
        "    imgsInSet = [] \n",
        "    \n",
        "    print(\"Starting to load subset of images in: \",imageSetPath)\n",
        "\n",
        "    #Read in all Images in the ImageSet\n",
        "    while (True):\n",
        "        line = imageSetFile.readline().splitlines()\n",
        "        #If end line - exit loop \n",
        "        if not line: \n",
        "            break\n",
        "\n",
        "        #Convert the file name to a clean path to the associated file\n",
        "        cleanLine = str(line)[1:-1].replace('\\'', '')\n",
        "        cleanPath = os.path.join(imageDirectory,'*/{}.png'.format(cleanLine))\n",
        "        fullPath = glob.glob(cleanPath)\n",
        "        cleanFullPath = str(fullPath)[1:-1].replace('\\'', '')\n",
        "        imgsInSet.append(cleanFullPath)\n",
        "        #print(cleanFullPath)\n",
        "\n",
        "    print(\"Starting to process images in: \",imageSetPath)\n",
        "\n",
        "    #Load the images\n",
        "    #Initialize the containers\n",
        "    numImages = len(imgsInSet)\n",
        "    x_data = numpy.empty([numImages, NUM_PIXELS])\n",
        "    y_data = numpy.empty([numImages,])\n",
        "\n",
        "    #For each image, get image in appropriate format for x_data \n",
        "        #and class for y_data\n",
        "    i = 0\n",
        "    for filename in imgsInSet:\n",
        "        #Reshape data from image file\n",
        "        x_data[i] = asarray(Image.open(filename)).flatten().reshape(1, -1)\n",
        "        \n",
        "        #Identify the class that the image is part of. Convert to int\n",
        "        filePath = os.path.dirname(filename)\n",
        "        className = os.path.basename(filePath)\n",
        "        j = 0\n",
        "        for c in CLASSES:\n",
        "            if (c == className):\n",
        "                y_data[i] = j\n",
        "                break\n",
        "            j = j + 1\n",
        "\n",
        "        i = i + 1\n",
        "    print(\"Done: \",imageSetPath)\n",
        "    #print(x_data)\n",
        "    y_data = y_data.astype(int)\n",
        "    #print(y_data)\n",
        "    return x_data, y_data\n",
        "\n",
        "#Test loading digits dataset for comparison\n",
        "def loadImagesTest():\n",
        "    digits = load_digits()\n",
        "    X_digits, y_digits = load_digits(return_X_y=True)\n",
        "    print(\"[loadImagesTest]: X_digits Type: \",format(type(X_digits)))\n",
        "    print(\"[loadImagesTest]: X_digits Shape: \",format(X_digits.shape))\n",
        "    print(\"[loadImagesTest]: X_digit: \",format(X_digits))\n",
        "\n",
        "    print(\"[loadImagesTest]: y_digits Type: \",format(type(y_digits)))\n",
        "    print(\"[loadImagesTest]: y_digits Shape: \",format(y_digits.shape))\n",
        "    print(\"[loadImagesTest]: y_digit: \",format(y_digits))\n",
        "    #print(\"feature_names: \", format(digits.feature_names))\n",
        "    #print(\"target_names: \", format(digits.target_names))\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)\n",
        "    print(X_train)\n",
        "    print(X_test)\n",
        "    print(y_train)\n",
        "    print(y_test)\n",
        "\n",
        "#loadImagesTest()\n",
        "\n",
        "#Save Resized images and Imagesets to google drive\n",
        "# drive.mount('https://drive.google.com/drive/shared-with-me')\n",
        "# imageDirectory = r'drive/u/0/shared-with-me/ResizedPNGImagesSmall'\n",
        "# imageSetsDirectory = r'drive/u/0/shared-with-me/ImageSets/Main'\n",
        "#Save Resized images and Imagesets to google drive\n",
        "drive.mount('/content/drive')\n",
        "imageDirectory = r'drive/My Drive/ResizedPNGImagesSmall/ResizedPNGImagesSmall'\n",
        "imageSetsDirectory = r'drive/My Drive/ImageSets/ImageSets/Main'\n",
        "\n",
        "\n",
        "imageSetTrainPath = os.path.join(imageSetsDirectory,'train.txt')\n",
        "imageSetValPath = os.path.join(imageSetsDirectory,'val.txt')\n",
        "\n",
        "x_train, y_train = loadDataSubSet(imageDirectory, imageSetTrainPath)\n",
        "x_test, y_test = loadDataSubSet(imageDirectory, imageSetValPath)\n",
        "\n",
        "log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\n",
        "print(\"Training\")\n",
        "print(log_reg.fit(x_train, y_train))\n",
        "print(\"End Training.... Starting Test\")\n",
        "print(log_reg.score(x_test, y_test))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Starting to load subset of images in:  drive/My Drive/ImageSets/ImageSets/Main/train.txt\n",
            "Starting to process images in:  drive/My Drive/ImageSets/ImageSets/Main/train.txt\n",
            "Done:  drive/My Drive/ImageSets/ImageSets/Main/train.txt\n",
            "Starting to load subset of images in:  drive/My Drive/ImageSets/ImageSets/Main/val.txt\n",
            "Starting to process images in:  drive/My Drive/ImageSets/ImageSets/Main/val.txt\n",
            "Done:  drive/My Drive/ImageSets/ImageSets/Main/val.txt\n",
            "Training\n",
            "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
            "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
            "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "End Training.... Starting Test\n",
            "0.5148548857976988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEPFWrl5s6l4"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38DpCGjOs9OF",
        "outputId": "759a2285-e696-4c27-a956-f6b5bedb3b22"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    (\"kmeans\", KMeans(n_clusters=70, random_state=42)),\n",
        "    (\"log_reg\", LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)),\n",
        "])\n",
        "pipeline.fit(x_train, y_train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
            "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
            "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('kmeans',\n",
              "                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\n",
              "                        max_iter=300, n_clusters=70, n_init=10, n_jobs=None,\n",
              "                        precompute_distances='auto', random_state=42,\n",
              "                        tol=0.0001, verbose=0)),\n",
              "                ('log_reg',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=5000,\n",
              "                                    multi_class='ovr', n_jobs=None,\n",
              "                                    penalty='l2', random_state=42,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAo8uajb7SCc",
        "outputId": "dadbff85-25f4-48bd-80ef-dd5108d41ca6"
      },
      "source": [
        "pipeline.score(x_test, y_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5320281641765413"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}